{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import argparse\n",
    "import numpy as np\n",
    "from fourrooms_2 import Fourrooms\n",
    "\n",
    "from scipy.special import expit\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "class Tabular:\n",
    "    def __init__(self, nstates):\n",
    "        self.nstates = nstates\n",
    "\n",
    "    def __call__(self, state):\n",
    "        return np.array([state,])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nstates\n",
    "\n",
    "class EgreedyPolicy:\n",
    "    def __init__(self, rng, nfeatures, nactions, epsilon):\n",
    "        self.rng = rng\n",
    "        self.epsilon = epsilon\n",
    "        self.weights = np.zeros((nfeatures, nactions))\n",
    "\n",
    "    def value(self, phi, action=None):\n",
    "        if action is None:\n",
    "            return np.sum(self.weights[phi, :], axis=0)\n",
    "        return np.sum(self.weights[phi, action], axis=0)\n",
    "\n",
    "    def sample(self, phi):\n",
    "        if self.rng.uniform() < self.epsilon:\n",
    "            return int(self.rng.randint(self.weights.shape[1]))\n",
    "        return int(np.argmax(self.value(phi)))\n",
    "\n",
    "class SoftmaxPolicy:\n",
    "    def __init__(self, rng, nfeatures, nactions, temp=1.):\n",
    "        self.rng = rng\n",
    "        self.weights = np.zeros((nfeatures, nactions))\n",
    "        self.temp = temp\n",
    "\n",
    "    def value(self, phi, action=None):\n",
    "        if action is None:\n",
    "            return np.sum(self.weights[phi, :], axis=0)\n",
    "        return np.sum(self.weights[phi, action], axis=0)\n",
    "\n",
    "    def pmf(self, phi):\n",
    "        v = self.value(phi)/self.temp\n",
    "        return np.exp(v - logsumexp(v))\n",
    "\n",
    "    def sample(self, phi):\n",
    "        return int(self.rng.choice(self.weights.shape[1], p=self.pmf(phi)))\n",
    "\n",
    "class SigmoidTermination:\n",
    "    def __init__(self, rng, nfeatures):\n",
    "        self.rng = rng\n",
    "        self.weights = np.zeros((nfeatures,))\n",
    "\n",
    "    def pmf(self, phi):\n",
    "        return expit(np.sum(self.weights[phi]))\n",
    "\n",
    "    def sample(self, phi):\n",
    "        return int(self.rng.uniform() < self.pmf(phi))\n",
    "\n",
    "    def grad(self, phi):\n",
    "        terminate = self.pmf(phi)\n",
    "        return terminate*(1. - terminate), phi\n",
    "\n",
    "class IntraOptionQLearning:\n",
    "    def __init__(self, discount, lr, terminations, weights):\n",
    "        self.lr = lr\n",
    "        self.discount = discount\n",
    "        self.terminations = terminations\n",
    "        self.weights = weights\n",
    "\n",
    "    def start(self, phi, option):\n",
    "        self.last_phi = phi\n",
    "        self.last_option = option\n",
    "        self.last_value = self.value(phi, option)\n",
    "\n",
    "    def value(self, phi, option=None):\n",
    "        if option is None:\n",
    "            return np.sum(self.weights[phi, :], axis=0)\n",
    "        return np.sum(self.weights[phi, option], axis=0)\n",
    "\n",
    "    def advantage(self, phi, option=None):\n",
    "        values = self.value(phi)\n",
    "        advantages = values - np.max(values)\n",
    "        if option is None:\n",
    "            return advantages\n",
    "        return advantages[option]\n",
    "\n",
    "    def update(self, phi, option, reward, done):\n",
    "        # One-step update target\n",
    "        update_target = reward\n",
    "        if not done:\n",
    "            current_values = self.value(phi)\n",
    "            termination = self.terminations[self.last_option].pmf(phi)\n",
    "            update_target += self.discount*((1. - termination)*current_values[self.last_option] + termination*np.max(current_values))\n",
    "\n",
    "        # Dense gradient update step\n",
    "        tderror = update_target - self.last_value\n",
    "        self.weights[self.last_phi, self.last_option] += self.lr*tderror\n",
    "\n",
    "        if not done:\n",
    "            self.last_value = current_values[option]\n",
    "        self.last_option = option\n",
    "        self.last_phi = phi\n",
    "\n",
    "        return update_target\n",
    "\n",
    "class IntraOptionActionQLearning:\n",
    "    def __init__(self, discount, lr, terminations, weights, qbigomega):\n",
    "        self.lr = lr\n",
    "        self.discount = discount\n",
    "        self.terminations = terminations\n",
    "        self.weights = weights\n",
    "        self.qbigomega = qbigomega\n",
    "\n",
    "    def value(self, phi, option, action):\n",
    "        return np.sum(self.weights[phi, option, action], axis=0)\n",
    "\n",
    "    def start(self, phi, option, action):\n",
    "        self.last_phi = phi\n",
    "        self.last_option = option\n",
    "        self.last_action = action\n",
    "\n",
    "    def update(self, phi, option, action, reward, done):\n",
    "        # One-step update target\n",
    "        update_target = reward\n",
    "        if not done:\n",
    "            current_values = self.qbigomega.value(phi)\n",
    "            termination = self.terminations[self.last_option].pmf(phi)\n",
    "            update_target += self.discount*((1. - termination)*current_values[self.last_option] + termination*np.max(current_values))\n",
    "\n",
    "        # Update values upon arrival if desired\n",
    "        tderror = update_target - self.value(self.last_phi, self.last_option, self.last_action)\n",
    "        self.weights[self.last_phi, self.last_option, self.last_action] += self.lr*tderror\n",
    "\n",
    "        self.last_phi = phi\n",
    "        self.last_option = option\n",
    "        self.last_action = action\n",
    "\n",
    "class TerminationGradient:\n",
    "    def __init__(self, terminations, critic, lr):\n",
    "        self.terminations = terminations\n",
    "        self.critic = critic\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, phi, option):\n",
    "        magnitude, direction = self.terminations[option].grad(phi)\n",
    "        self.terminations[option].weights[direction] -= \\\n",
    "                self.lr*magnitude*(self.critic.advantage(phi, option))\n",
    "\n",
    "class IntraOptionGradient:\n",
    "    def __init__(self, option_policies, lr):\n",
    "        self.lr = lr\n",
    "        self.option_policies = option_policies\n",
    "\n",
    "    def update(self, phi, option, action, critic):\n",
    "        actions_pmf = self.option_policies[option].pmf(phi)\n",
    "        self.option_policies[option].weights[phi, :] -= self.lr*critic*actions_pmf\n",
    "        self.option_policies[option].weights[phi, action] += self.lr*critic\n",
    "\n",
    "class OneStepTermination:\n",
    "    def sample(self, phi):\n",
    "        return 1\n",
    "\n",
    "    def pmf(self, phi):\n",
    "        return 1.\n",
    "\n",
    "class FixedActionPolicies:\n",
    "    def __init__(self, action, nactions):\n",
    "        self.action = action\n",
    "        self.probs = np.eye(nactions)[action]\n",
    "\n",
    "    def sample(self, phi):\n",
    "        return self.action\n",
    "\n",
    "    def pmf(self, phi):\n",
    "        return self.probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--primitive'], dest='primitive', nargs=0, const=True, default=False, type=None, choices=None, help='Augment with primitive', metavar=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--discount', help='Discount factor', type=float, default=0.99)\n",
    "parser.add_argument('--lr_term', help=\"Termination gradient learning rate\", type=float, default=1e-3)\n",
    "parser.add_argument('--lr_intra', help=\"Intra-option gradient learning rate\", type=float, default=1e-3)\n",
    "parser.add_argument('--lr_critic', help=\"Learning rate\", type=float, default=1e-2)\n",
    "parser.add_argument('--epsilon', help=\"Epsilon-greedy for policy over options\", type=float, default=1e-2)\n",
    "#     parser.add_argument('--nepisodes', help=\"Number of episodes per run\", type=int, default=250)\n",
    "parser.add_argument('--nepisodes', help=\"Number of episodes per run\", type=int, default=10)\n",
    "#     parser.add_argument('--nruns', help=\"Number of runs\", type=int, default=100)\n",
    "parser.add_argument('--nruns', help=\"Number of runs\", type=int, default=10)\n",
    "parser.add_argument('--nsteps', help=\"Maximum number of steps per episode\", type=int, default=1000)\n",
    "parser.add_argument('--noptions', help='Number of options', type=int, default=4)\n",
    "parser.add_argument('--baseline', help=\"Use the baseline for the intra-option gradient\", action='store_true', default=False)\n",
    "parser.add_argument('--temperature', help=\"Temperature parameter for softmax\", type=float, default=1e-2)\n",
    "parser.add_argument('--primitive', help=\"Augment with primitive\", default=False, action='store_true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[1.20074501e-11 4.94254609e-07 0.00000000e+00 2.43144588e-09]\n",
      "[1.13867883e-19 0.00000000e+00 2.92828711e-18 8.46324170e-20]\n",
      "[0. 0. 0. 0.]\n",
      "[1.19690873e-08 1.91156210e-06 3.80486323e-04 9.53692748e-07]\n",
      "Run 0 episode 9 steps 945 cumreward 1.0 avg. duration 2.9748953974895422 switches 478\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-34804207109e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moption_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;31m# Critic update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-6169c1e00779>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, phi)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpmf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mSigmoidTermination\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-6169c1e00779>\u001b[0m in \u001b[0;36mpmf\u001b[0;34m(self, phi)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpmf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-6169c1e00779>\u001b[0m in \u001b[0;36mvalue\u001b[0;34m(self, phi, action)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2092\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0marray_function_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sum_dispatcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2093\u001b[0m def sum(a, axis=None, dtype=None, out=None, keepdims=np._NoValue,\n\u001b[1;32m   2094\u001b[0m         initial=np._NoValue, where=np._NoValue):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = parser.parse_args(args={})\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "env = Fourrooms()\n",
    "\n",
    "fname = '-'.join(['{}_{}'.format(param, val) for param, val in sorted(vars(args).items())])\n",
    "fname = 'optioncritic-fourrooms-' + fname + '.npy'\n",
    "\n",
    "possible_next_goals = [68, 69, 70, 71, 72, 78, 79, 80, 81, 82, 88, 89, 90, 91, 92, 93, 99, 100, 101, 102, 103]\n",
    "\n",
    "history = np.zeros((args.nruns, args.nepisodes, 2))\n",
    "for run in range(args.nruns):\n",
    "    features = Tabular(env.observation_space.n)\n",
    "    nfeatures, nactions = len(features), env.action_space.n\n",
    "\n",
    "    # The intra-option policies are linear-softmax functions\n",
    "    option_policies = [SoftmaxPolicy(rng, nfeatures, nactions, args.temperature) for _ in range(args.noptions)]\n",
    "    if args.primitive:\n",
    "        option_policies.extend([FixedActionPolicies(act, nactions) for act in range(nactions)])\n",
    "\n",
    "    # The termination function are linear-sigmoid functions\n",
    "    option_terminations = [SigmoidTermination(rng, nfeatures) for _ in range(args.noptions)]\n",
    "    if args.primitive:\n",
    "        option_terminations.extend([OneStepTermination() for _ in range(nactions)])\n",
    "\n",
    "    # E-greedy policy over options\n",
    "    #policy = EgreedyPolicy(rng, nfeatures, args.noptions, args.epsilon)\n",
    "    policy = SoftmaxPolicy(rng, nfeatures, args.noptions, args.temperature)\n",
    "\n",
    "    # Different choices are possible for the critic. Here we learn an\n",
    "    # option-value function and use the estimator for the values upon arrival\n",
    "    critic = IntraOptionQLearning(args.discount, args.lr_critic, option_terminations, policy.weights)\n",
    "\n",
    "    # Learn Qomega separately\n",
    "    action_weights = np.zeros((nfeatures, args.noptions, nactions))\n",
    "    action_critic = IntraOptionActionQLearning(args.discount, args.lr_critic, option_terminations, action_weights, critic)\n",
    "\n",
    "    # Improvement of the termination functions based on gradients\n",
    "    termination_improvement= TerminationGradient(option_terminations, critic, args.lr_term)\n",
    "\n",
    "    # Intra-option gradient improvement with critic estimator\n",
    "    intraoption_improvement = IntraOptionGradient(option_policies, args.lr_intra)\n",
    "\n",
    "    for episode in range(args.nepisodes):\n",
    "        if episode == 1000:\n",
    "            env.goal = rng.choice(possible_next_goals)\n",
    "            print('************* New goal : ', env.goal)\n",
    "\n",
    "        phi = features(env.reset())\n",
    "        print(policy.value(phi))\n",
    "        option = policy.sample(phi)\n",
    "        action = option_policies[option].sample(phi)\n",
    "        critic.start(phi, option)\n",
    "        action_critic.start(phi, option, action)\n",
    "\n",
    "        cumreward = 0.\n",
    "        duration = 1\n",
    "        option_switches = 0\n",
    "        avgduration = 0.\n",
    "        for step in range(args.nsteps):\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            phi = features(observation)\n",
    "\n",
    "            # Termination might occur upon entering the new state\n",
    "            if option_terminations[option].sample(phi):\n",
    "                option = policy.sample(phi)\n",
    "                option_switches += 1\n",
    "                avgduration += (1./option_switches)*(duration - avgduration)\n",
    "                duration = 1\n",
    "\n",
    "            action = option_policies[option].sample(phi)\n",
    "\n",
    "            # Critic update\n",
    "            update_target = critic.update(phi, option, reward, done)\n",
    "            action_critic.update(phi, option, action, reward, done)\n",
    "\n",
    "            if isinstance(option_policies[option], SoftmaxPolicy):\n",
    "                # Intra-option policy update\n",
    "                critic_feedback = action_critic.value(phi, option, action)\n",
    "                if args.baseline:\n",
    "                    critic_feedback -= critic.value(phi, option)\n",
    "                intraoption_improvement.update(phi, option, action, critic_feedback)\n",
    "\n",
    "                # Termination update\n",
    "                termination_improvement.update(phi, option)\n",
    "\n",
    "            cumreward += reward\n",
    "            duration += 1\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        history[run, episode, 0] = step\n",
    "        history[run, episode, 1] = avgduration\n",
    "\n",
    "    print('Run {} episode {} steps {} cumreward {} avg. duration {} switches {}'.format(run, episode, step, cumreward, avgduration, option_switches))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
